SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1
C:\Users\Shreeyut\AppData\Roaming\Python\Python313\site-packages\torch\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\TensorShape.cpp:3638.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
---final upsample expand_first---
loading data from the directory: C:\Users\Shreeyut\deep-learning-lab\UNET-SWIN\SwinUNET_Flash_Attention\network\STARCOP\ang20190922t192642_r2048_c0_w512_h512-20250410T125838Z-001
Found 1 valid image-mask pairs
torch.Size([1, 4, 224, 224])
torch.Size([1, 4, 224, 224])
Traceback (most recent call last):
  File "c:\Users\Shreeyut\deep-learning-lab\UNET-SWIN\SwinUNET_Flash_Attention\network\trainer.py", line 59, in <module>
    loss = criterion(output, mask)
  File "C:\Users\Shreeyut\AppData\Roaming\Python\Python313\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\Shreeyut\AppData\Roaming\Python\Python313\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\Shreeyut\AppData\Roaming\Python\Python313\site-packages\torch\nn\modules\loss.py", line 821, in forward
    return F.binary_cross_entropy_with_logits(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<3 lines>...
        reduction=self.reduction,
        ^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\Shreeyut\AppData\Roaming\Python\Python313\site-packages\torch\nn\functional.py", line 3639, in binary_cross_entropy_with_logits
    raise ValueError(
        f"Target size ({target.size()}) must be the same as input size ({input.size()})"
    )
ValueError: Target size (torch.Size([1, 4, 224, 224])) must be the same as input size (torch.Size([1, 1, 224, 224]))
